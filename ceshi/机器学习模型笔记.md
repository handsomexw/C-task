```python
split（）
#用于分割字符串，参数是指出按什么标识符进行分割，也可以用来分离图片rgb三通道
append（）
#用于尾插字符串
abs（）
#用于返回绝对值
sign（）
#用来保障所求数是正数
os.listdir
#用于读取文件和文件夹名字并存放在列表中
np.array()
用于创建数组
```

机器学习模模型参数设置

- 决策树

  - DecisionTreeClassifier的典型参数

        criterion（特征选择标准）
        可选择ceriterion=‘gini’ 或ceriterion=‘entropy’
        前者是基尼系数，后者是信息熵。两者差别不大，默认使用gini系数
        splitter（特征划分标准）
        可选择splitter='best’或splitter=‘random’
        前者在特征的所有划分点中找出最优的划分点。后者是随机的在部分划分点中找局部最优的划分点。默认使用best，它适合样本量不大的情况，若样本量较大，推荐使用random
        max_depth（决策树最大深度）
        默认为Non，可赋int值。
        在样本量多，特征多的时候，可设置max_depth来解决过拟合问题。
        min_impurity_decrease（节点划分最小不纯度）
        默认为0，可赋float值。
        这个值限制了决策树的增长，如果某节点的不纯度(基尼系数，信息增益，均方差，绝对差)小于这个阈值，则该节点不再生成子节点。
        min_samples_split（内部节点再划分所需最小样本数）
        默认为2，可赋int或float值。若为float则向上取整
        顾名思义,但在样本数较少时几乎没有影响
        min_samples_leaf（叶子节点最少样本数）
        可赋int或float值
        这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。
        max_leaf_nodes（最大叶子节点数）
        默认为0，可赋int值
        通过限制最大叶子节点数，可以防止过拟合，默认是”None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制，具体的值可以通过交叉验证得到。
        min_impurity_split（信息增益的阀值）
        决策树在创建分支时，信息增益必须大于这个阀值，否则不分裂
        min_weight_fraction_leaf（叶子节点最小的样本权重和）
        默认为0，可赋float值
        这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。 一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。
        class_weight（类别权重）
        默认是non，可选择class_weight=‘balanced’
        指定样本各类别的的权重，主要是为了防止训练集某些类别的样本过多，导致训练的决策树过于偏向这些类别。如果使用“balanced”，则算法会自己计算权重，样本量少的类别所对应的样本权重会高。当然，如果你的样本类别分布没有明显的偏倚，则可以不管这个参数。
        另外，non不适用与回归树。

- k近邻

  - n_neighbors：int型，默认为5。即k值。

    weights: {‘uniform’, ‘distance’} or callable, default=’uniform’ ，邻居们标签的权重。

    algorithm：{‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, default=’auto’，搜索邻居的算法。

    leaf_size：int, default=30，叶节点数量，如上所述, 对于小样本暴力搜索是比基于树的搜索更有效的方法. 这一事实在 ball 树和 KD 树中被解释为在叶节点内部切换到暴力搜索。

    p：int, default=2，距离，当p=1时是曼哈顿距离，当p=2时是欧氏距离。

    metric：str 或 callable, default=‘minkowski’，用于树的距离度量。默认度量是 minkowski，并且 p=2 等效于标准欧几里得度量。也可以自己编写距离函数。

    metric_param：dict，默认值=None，距离度量的其他参数。

    n_jobs：int，默认=无，工作cpu的个数。

- svm

  - C: 惩罚系数，用来控制损失函数的惩罚系数，类似于LR中的正则化系数。C越大，相当于惩罚松弛变量，希望松弛变量接近0，即对误分类的惩罚增大，趋向于对训练集全分对的情况，这样会出现训练集测试时准确率很高，但泛化能力弱，容易导致过拟合。
     C值小，对误分类的惩罚减小，容错能力增强，泛化能力较强，但也可能欠拟合。
  - kernel: 算法中采用的和函数类型，核函数是用来将非[线性](https://so.csdn.net/so/search?q=线性&spm=1001.2101.3001.7020)问题转化为线性问题的一种方法。参数选择有RBF, Linear, Poly,  Sigmoid，precomputed或者自定义一个核函数,默认的是"RBF"，即径向基核，也就是高斯核函数；而Linear指的是线性核函数，Poly指的是多项式核，Sigmoid指的是双曲正切函数tanh核；
  - degree:
     当指定kernel为’poly’时，表示选择的多项式的最高次数，默认为三次多项式；若指定kernel不是’poly’，则忽略，即该参数只对’poly’有用。（多项式核函数是将低维的输入空间映射到高维的特征空间）
  - gamma: 核函数系数，该参数是rbf，poly和sigmoid的内核系数；默认是’auto’，那么将会使用特征位数的倒数，即1 /  n_features。（即核函数的带宽，超圆的半径）。gamma越大，σ越小，使得高斯分布又高又瘦，造成模型只能作用于支持向量附近，可能导致过拟合；反之，gamma越小，σ越大，高斯分布会过于平滑，在训练集上分类效果不佳，可能导致欠拟合。
  - coef0: 核函数常数值(y=kx+b中的b值)，只有‘poly’和‘sigmoid’核函数有，默认值是0。

- 神经网络

  - 参数说明:

        hidden_layer_sizes :例如hidden_layer_sizes=(50, 50)，表示有两层隐藏层，第一层隐藏层有50个神经元，第二层也有50个神经元。
        
        activation :激活函数,{‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, 默认relu
        
        identity：f(x) = x
        
        logistic：其实就是sigmod,f(x) = 1 / (1 + exp(-x)).
        
        tanh：f(x) = tanh(x).
        
        relu：f(x) = max(0, x)
        
        solver： {‘lbfgs’, ‘sgd’, ‘adam’}, 默认adam，用来优化权重
        
        lbfgs：quasi-Newton方法的优化器
        
        sgd：随机梯度下降
        
        adam： Kingma, Diederik, and Jimmy Ba提出的机遇随机梯度的优化器

    注意：默认solver ‘adam’在相对较大的数据集上效果比较好（几千个样本或者更多），对小数据集来说，lbfgs收敛更快效果也更好。

        alpha :float,可选的，默认0.0001,正则化项参数
        
        batch_size : int , 可选的，默认’auto’,随机优化的minibatches的大小batch_size=min(200,n_samples)，如果solver是’lbfgs’，分类器将不使用minibatch
        
        learning_rate :学习率,用于权重更新,只有当solver为’sgd’时使用，{‘constant’，’invscaling’, ‘adaptive’},默认constant
        
        ‘constant’: 有’learning_rate_init’给定的恒定学习率
        
        ‘incscaling’：随着时间t使用’power_t’的逆标度指数不断降低学习率learning_rate_ ，effective_learning_rate = learning_rate_init / pow(t, power_t)
        
        ‘adaptive’：只要训练损耗在下降，就保持学习率为’learning_rate_init’不变，当连续两次不能降低训练损耗或验证分数停止升高至少tol时，将当前学习率除以5.
        
        power_t: double, 可选, default 0.5，只有solver=’sgd’时使用，是逆扩展学习率的指数.当learning_rate=’invscaling’，用来更新有效学习率。
        
        max_iter: int，可选，默认200，最大迭代次数。
        
        random_state:int 或RandomState，可选，默认None，随机数生成器的状态或种子。
        
        shuffle: bool，可选，默认True,只有当solver=’sgd’或者‘adam’时使用，判断是否在每次迭代时对样本进行清洗。
        
        tol：float, 可选，默认1e-4，优化的容忍度
        
        learning_rate_int:double,可选，默认0.001，初始学习率，控制更新权重的补偿，只有当solver=’sgd’ 或’adam’时使用。
        
        verbose : bool, 可选, 默认False,是否将过程打印到stdout
        
        warm_start : bool, 可选, 默认False,当设置成True，使用之前的解决方法作为初始拟合，否则释放之前的解决方法。
        
        momentum : float, 默认 0.9,动量梯度下降更新，设置的范围应该0.0-1.0. 只有solver=’sgd’时使用.
        
        nesterovs_momentum : boolean, 默认True, Whether to use Nesterov’s momentum. 只有solver=’sgd’并且momentum > 0使用.
        
        early_stopping : bool, 默认False,只有solver=’sgd’或者’adam’时有效,判断当验证效果不再改善的时候是否终止训练，当为True时，自动选出10%的训练数据用于验证并在两步连续迭代改善，低于tol时终止训练。
        
        validation_fraction : float, 可选, 默认 0.1,用作早期停止验证的预留训练数据集的比例，早0-1之间，只当early_stopping=True有用
        
        beta_1 : float, 可选, 默认0.9，只有solver=’adam’时使用，估计一阶矩向量的指数衰减速率，[0,1)之间
        
        beta_2 : float, 可选, 默认0.999,只有solver=’adam’时使用估计二阶矩向量的指数衰减速率[0,1)之间
        
        epsilon : float, 可选, 默认1e-8,只有solver=’adam’时使用数值稳定值。
    

- 混淆矩阵
  - 一般用于比较结果与实际测得值，可以把分类结果的精度显示在一个混淆矩阵里面
  - 混淆矩阵要表达的含义：
    - 混淆矩阵的每一列代表了预测类别，每一列的总数表示预测为该类别的数据的数目；
    - 每一行代表了数据的真实归属类别，每一行的数据总数表示该类别的数据实例的数目；每一列中的数值表示真实数据被预测为该类的数目。

